{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "VJUb_7D2TerM"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import subprocess\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "EAKwFe1mTfqp"
   },
   "outputs": [],
   "source": [
    "class MLAPI:\n",
    "    device = None\n",
    "    nvidia = False\n",
    "    dataset_name = None\n",
    "    dataset = None\n",
    "    model = None\n",
    "    accuracy = 0.0\n",
    "    \n",
    "    def __init__(self, checkpoint=\"\"):\n",
    "        try:\n",
    "            subprocess.check_output('nvidia-smi')\n",
    "            nvidia = True\n",
    "            print(\"Nvidia drivers available!\")\n",
    "        except Exception: \n",
    "            # this command not being found can raise quite a few \n",
    "            # different errors depending on the configuration\n",
    "            print('No Nvidia GPU in system!')\n",
    "        try:\n",
    "            self.device = torch.device('cuda:0')\n",
    "            print(\"GPU available!\")\n",
    "        except:\n",
    "            print(\"No GPU available in system!\")\n",
    "        \n",
    "    def __str__(self):\n",
    "        try:\n",
    "            return self.accuracy\n",
    "        except:\n",
    "            return 0\n",
    "\n",
    "    def one_hot_encode(self, inplace=True, dataset=None, cutoff = 10, columns=None):\n",
    "        # TODO: Fix this\n",
    "        \n",
    "        # do a one hot encode of the current dataset and then return \n",
    "        # the new column names alongside the new dataframe\n",
    "        print(\"TESTING\")\n",
    "        try:\n",
    "            print(\"Columns: \")\n",
    "            print ([self.dataset[col].unique().tolist() for col in self.dataset.columns])\n",
    "            # one_hot_df = pd.get_dummies(self.dataset)\n",
    "        \n",
    "            # new_columns = one_hot_df.columns.difference(self.dataset.columns).tolist()\n",
    "            \n",
    "        # catch when df1 is None\n",
    "        except AttributeError as e:\n",
    "            print(e)\n",
    "            return None, None\n",
    "        # catch when it hasn't even been defined\n",
    "        except NameError as e:\n",
    "            print(e)\n",
    "            return None, None\n",
    "            \n",
    "        if inplace:\n",
    "            self.dataset = one_hot_df\n",
    "            \n",
    "        return one_hot_df, new_columns\n",
    "    \n",
    "    def set_local_csv_dataset(self, dataset=None, encoding=None):\n",
    "        self.dataset_name = dataset\n",
    "        if dataset == None:\n",
    "            from sklearn import datasets\n",
    "            iris = datasets.load_iris()\n",
    "            irisdf = pd.DataFrame(data= np.c_[iris['data'], iris['target']],\n",
    "                                 columns= iris['feature_names'] + ['target'])\n",
    "            self.dataset = irisdf\n",
    "            \n",
    "        if dataset == \"social_network\":\n",
    "            self.dataset = pd.read_csv(\"../Data/Social_Network_Ads.csv\")\n",
    "            \n",
    "        else:\n",
    "            try:\n",
    "                if encoding != None:\n",
    "                    self.dataset = pd.read_csv(self.dataset_name, encoding=encoding)\n",
    "                else:\n",
    "                    self.dataset = pd.read_csv(self.dataset_name)\n",
    "            except:\n",
    "                print(\"Not CSV\")\n",
    "                return False\n",
    "        return True\n",
    "\n",
    "    def set_local_json_dataset(self, dataset):\n",
    "        pass\n",
    "\n",
    "\n",
    "    \n",
    "    def calculate_r2(self, y_true, y_pred):\n",
    "        \"\"\"\n",
    "        Calculate the R² score directly using predictions.\n",
    "        \n",
    "        Parameters:\n",
    "        y_true (array-like): True target values.\n",
    "        y_pred (array-like): Predicted target values from a simple linear model.\n",
    "        \n",
    "        Returns:\n",
    "        float: R² score.\n",
    "        \"\"\"\n",
    "        ss_res = np.sum((y_true - y_pred) ** 2)  # Residual sum of squares\n",
    "        ss_tot = np.sum((y_true - np.mean(y_true)) ** 2)  # Total sum of squares\n",
    "        r2 = 1 - (ss_res / ss_tot)\n",
    "        return r2\n",
    "        \n",
    "    def recommed_model(self, columns, features):\n",
    "        # should probably mostly rely upon user input to recommend a model\n",
    "        # if data is linear and wants easy explanation -> linear\n",
    "        # linear data + categorical data -> logistic regression\n",
    "        # we can get linearity from R^2 score?\n",
    "        # high dimensional data + easy explanation or large amount of entries -> decision tree\n",
    "        # high dimensional data + higher accuracy + fewer entries -> SVM\n",
    "        # SVM takes longer to train than most trees\n",
    "        # TODO: add naive bayes algorithm to class\n",
    "\n",
    "        X = self.dataset[features].values\n",
    "        y = self.dataset[columns].values.flatten()\n",
    "\n",
    "        coefficients = np.linalg.lstsq(X, y, rcond=None)[0]  # Least squares solution\n",
    "        y_pred = np.dot(X, coefficients)\n",
    "        print(y, y_pred)\n",
    "        r2 = self.calculate_r2(y, y_pred)\n",
    "\n",
    "        if r2 > 0.7:\n",
    "            return \"Linear Regression: Data is highly linear.\"\n",
    "        elif len(np.unique(y)) == 2:\n",
    "            return \"Logistic Regression: Linear data with binary outcome.\"\n",
    "        \n",
    "            \n",
    "\n",
    "\n",
    "        if X.shape[1] > 10:  # Arbitrary threshold for high-dimensional data\n",
    "            return \"Decision Tree Classifier: High-dimensional data with many entries.\"\n",
    "        else:\n",
    "            return \"SVM: High-dimensional data with fewer entries, aiming for higher accuracy.\"\n",
    "\n",
    "        \n",
    "        \n",
    "        pass\n",
    "        \n",
    "    def logistic_regression(self,label, lr=1e-4, test_size=0.25, random_state=42, columns=None, max_epochs=100, verbose=1):\n",
    "        df_encoded = pd.get_dummies(self.dataset, drop_first=True)\n",
    "        y = self.dataset[label]\n",
    "        if columns == None:\n",
    "            #assume all other columns and set X_columns to all features not label\n",
    "            X_columns = [col for col in self.dataset.columns if label not in col]\n",
    "        else:\n",
    "            # use only given columns\n",
    "            X_columns = columns\n",
    "        X = self.dataset[X_columns]\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "        self.model = LogisticRegression(max_iter=max_epochs, verbose = verbose)\n",
    "        self.model.fit(X_train, y_train)\n",
    "        self.accuracy = self.model.score(X_test, y_test)\n",
    "        print(f\"Model Accuracy: {self.accuracy}\")\n",
    "        return self.model\n",
    "\n",
    "    def linear_regression(self,label, lr=1e-4, test_size=0.25, random_state=42, columns=None, max_epochs=100, verbose=1):\n",
    "        # worth noting that the linear regression method via sklearn is very sparsely implemented\n",
    "        # takes almost no method arguments\n",
    "        df_encoded = pd.get_dummies(self.dataset, drop_first=True)\n",
    "\n",
    "        #TODO: get columns from df_encoded if categorical data that is one hot encoded\n",
    "        \n",
    "        y = df_encoded[label]\n",
    "        if columns == None:\n",
    "            #assume all other columns and set X_columns to all features not label\n",
    "            X_columns = [col for col in df_encoded.columns if label not in col]\n",
    "        else:\n",
    "            # use only given columns\n",
    "            X_columns = columns\n",
    "        X = df_encoded[X_columns]\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "        self.model = LinearRegression()\n",
    "        self.model.fit(X_train, y_train)\n",
    "        self.accuracy = self.model.score(X_test, y_test)\n",
    "        print(f\"Model Accuracy: {self.accuracy}\")\n",
    "        return self.model\n",
    "\n",
    "    def svm(self,label, lr=1e-4, test_size=0.25, random_state=42, columns=None, max_epochs=100, verbose=1):\n",
    "        y = self.dataset[label]\n",
    "        X_columns = None\n",
    "        if columns == None:\n",
    "            #assume all other columns and set X_columns to all features not label\n",
    "            X_columns = [col for col in self.dataset.columns if label not in col]\n",
    "        else:\n",
    "            X_columns = columns\n",
    "            \n",
    "        X = self.dataset[X_columns]\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = test_size, random_state = random_state)\n",
    "        sc = StandardScaler()\n",
    "        X_train = sc.fit_transform(X_train)\n",
    "        X_test = sc.transform(X_test)\n",
    "        self.model = SVC(kernel = 'rbf', random_state = random_state, verbose = verbose)\n",
    "        self.model.fit(X_train, y_train)\n",
    "        y_pred = self.model.predict(X_test)\n",
    "        self.accuracy = accuracy_score(y_test,y_pred)\n",
    "        print(f\"Model Accuracy: {self.accuracy}\")\n",
    "        return self.model\n",
    "\n",
    "    def decision_tree(self,label, lr=1e-4, test_size=0.25, random_state=42, columns=None, max_epochs=100, verbose=1):\n",
    "        y = self.dataset[label]\n",
    "        if columns == None:\n",
    "            #assume all other columns and set X_columns to all features not label\n",
    "            X_columns = [col for col in self.dataset.columns if label not in col]\n",
    "        else:\n",
    "            # use only given columns\n",
    "            X_columns = columns\n",
    "        X = self.dataset[X_columns]\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "\n",
    "        scaler = StandardScaler()\n",
    "        try:\n",
    "            X_train_scaled = scaler.fit_transform(X_train)\n",
    "        except:\n",
    "            X_train_scaled = X_train\n",
    "\n",
    "        try:\n",
    "            X_test_scaled = scaler.transform(X_test)\n",
    "        except:\n",
    "            X_test_scaled = X_test\n",
    "\n",
    "        #TODO: Add verbosity to decision tree classifier?\n",
    "        self.model = DecisionTreeClassifier(random_state = random_state)\n",
    "        self.model.fit(X_train, y_train)\n",
    "        y_pred = self.model.predict(X_test)\n",
    "        self.accuracy = self.model.score(X_test, y_test)\n",
    "        print(f\"Model Accuracy: {self.accuracy}\")\n",
    "        return self.model\n",
    "\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook ML_API.ipynb to script\n",
      "[NbConvertApp] Writing 9681 bytes to ML_API.py\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbconvert --to script 'ML_API.ipynb'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "L4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
