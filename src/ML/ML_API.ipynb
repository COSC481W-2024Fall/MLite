{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "VJUb_7D2TerM"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import subprocess\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "import argparse\n",
    "from sklearn.neural_network import MLPClassifier, MLPRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "EAKwFe1mTfqp"
   },
   "outputs": [],
   "source": [
    "class MLAPI:\n",
    "    device = None\n",
    "    nvidia = False\n",
    "    dataset_name = None\n",
    "    dataset = None\n",
    "    model = None\n",
    "    accuracy = 0.0\n",
    "    \n",
    "    def __init__(self, checkpoint=\"\"):\n",
    "        try:\n",
    "            subprocess.check_output('nvidia-smi')\n",
    "            nvidia = True\n",
    "            print(\"Nvidia drivers available!\")\n",
    "        except Exception: \n",
    "            # this command not being found can raise quite a few \n",
    "            # different errors depending on the configuration\n",
    "            print('No Nvidia GPU in system!')\n",
    "        try:\n",
    "            self.device = torch.device('cuda:0')\n",
    "            print(\"GPU available!\")\n",
    "        except:\n",
    "            print(\"No GPU available in system!\")\n",
    "        \n",
    "    def __str__(self):\n",
    "        try:\n",
    "            return self.accuracy\n",
    "        except:\n",
    "            return 0\n",
    "\n",
    "    def one_hot_encode(self, inplace=True, dataset=None, cutoff = 10, columns=None):\n",
    "        # TODO: Fix this\n",
    "        \n",
    "        # do a one hot encode of the current dataset and then return \n",
    "        # the new column names alongside the new dataframe\n",
    "        print(\"TESTING\")\n",
    "        try:\n",
    "            print(\"Columns: \")\n",
    "            print ([self.dataset[col].unique().tolist() for col in self.dataset.columns])\n",
    "            one_hot_df = pd.get_dummies(self.dataset)\n",
    "        \n",
    "            new_columns = one_hot_df.columns.difference(self.dataset.columns).tolist()\n",
    "            \n",
    "        # catch when df1 is None\n",
    "        except AttributeError as e:\n",
    "            print(e)\n",
    "            return None, None\n",
    "        # catch when it hasn't even been defined\n",
    "        except NameError as e:\n",
    "            print(e)\n",
    "            return None, None\n",
    "            \n",
    "        if inplace:\n",
    "            self.dataset = one_hot_df\n",
    "            \n",
    "        return one_hot_df, new_columns\n",
    "    \n",
    "    def set_local_csv_dataset(self, dataset=None, encoding=None, concat=False):\n",
    "        self.dataset_name = dataset\n",
    "        if dataset == None:\n",
    "            from sklearn import datasets\n",
    "            iris = datasets.load_iris()\n",
    "            irisdf = pd.DataFrame(data= np.c_[iris['data'], iris['target']],\n",
    "                                 columns= iris['feature_names'] + ['target'])\n",
    "            self.dataset = irisdf\n",
    "            \n",
    "        if dataset == \"social_network\":\n",
    "            self.dataset = pd.read_csv(\"../Data/Social_Network_Ads.csv\")\n",
    "            \n",
    "        else:\n",
    "            try:\n",
    "                if concat == True:\n",
    "                    if encoding != None:\n",
    "                        new_dataset = pd.read_csv(self.dataset_name, encoding=encoding)\n",
    "                    else:\n",
    "                        new_dataset = pd.read_csv(self.dataset_name)\n",
    "                    self.dataset = pd.concat([self.dataset, new_dataset])\n",
    "                if encoding != None:\n",
    "                    self.dataset = pd.read_csv(self.dataset_name, encoding=encoding)\n",
    "                else:\n",
    "                    self.dataset = pd.read_csv(self.dataset_name)\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                return False\n",
    "        return True\n",
    "\n",
    "    def set_local_json_dataset(self, dataset):\n",
    "        pass\n",
    "\n",
    "\n",
    "    \n",
    "    def calculate_r2(self, y_true, y_pred):\n",
    "        \"\"\"\n",
    "        Calculate the R² score directly using predictions.\n",
    "        \n",
    "        Parameters:\n",
    "        y_true (array-like): True target values.\n",
    "        y_pred (array-like): Predicted target values from a simple linear model.\n",
    "        \n",
    "        Returns:\n",
    "        float: R² score.\n",
    "        \"\"\"\n",
    "        ss_res = np.sum((y_true - y_pred) ** 2)  # Residual sum of squares\n",
    "        ss_tot = np.sum((y_true - np.mean(y_true)) ** 2)  # Total sum of squares\n",
    "        r2 = 1 - (ss_res / ss_tot)\n",
    "        return r2\n",
    "        \n",
    "    def recommed_model(self, columns, features):\n",
    "        # should probably mostly rely upon user input to recommend a model\n",
    "        # if data is linear and wants easy explanation -> linear\n",
    "        # linear data + categorical data -> logistic regression\n",
    "        # we can get linearity from R^2 score?\n",
    "        # high dimensional data + easy explanation or large amount of entries -> decision tree\n",
    "        # high dimensional data + higher accuracy + fewer entries -> SVM\n",
    "        # SVM takes longer to train than most trees\n",
    "        # TODO: add naive bayes algorithm to class\n",
    "\n",
    "        X = self.dataset[features].values\n",
    "        y = self.dataset[columns].values.flatten()\n",
    "\n",
    "        coefficients = np.linalg.lstsq(X, y, rcond=None)[0]  # Least squares solution\n",
    "        y_pred = np.dot(X, coefficients)\n",
    "        print(y, y_pred)\n",
    "        r2 = self.calculate_r2(y, y_pred)\n",
    "\n",
    "        if r2 > 0.7:\n",
    "            return \"Linear Regression: Data is highly linear.\"\n",
    "        elif len(np.unique(y)) == 2:\n",
    "            return \"Logistic Regression: Linear data with binary outcome.\"\n",
    "        \n",
    "            \n",
    "\n",
    "\n",
    "        if X.shape[1] > 10:  # Arbitrary threshold for high-dimensional data\n",
    "            return \"Decision Tree Classifier: High-dimensional data with many entries.\"\n",
    "        else:\n",
    "            return \"SVM: High-dimensional data with fewer entries, aiming for higher accuracy.\"\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "    def logistic_regression(self,label=None, lr=1e-4, test_size=0.25, random_state=42, columns=None, max_epochs=100, verbose=1):\n",
    "        df_encoded = pd.get_dummies(self.dataset, drop_first=True)\n",
    "        if label == None:\n",
    "            y = self.dataset[-1]\n",
    "        else:\n",
    "            y = self.dataset[label]\n",
    "            \n",
    "        if columns == None:\n",
    "            #assume all other columns and set X_columns to all features not label\n",
    "            X_columns = [col for col in self.dataset.columns if label not in col]\n",
    "        else:\n",
    "            # use only given columns\n",
    "            X_columns = columns\n",
    "        X = self.dataset[X_columns]\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "        self.model = LogisticRegression(max_iter=max_epochs, verbose = verbose)\n",
    "        self.model.fit(X_train, y_train)\n",
    "        self.accuracy = self.model.score(X_test, y_test)\n",
    "        print(f\"Model Accuracy: {self.accuracy}\")\n",
    "        return self.model\n",
    "\n",
    "    def linear_regression(self,label=None, lr=1e-4, test_size=0.25, random_state=42, columns=None, max_epochs=100, verbose=1):\n",
    "        # worth noting that the linear regression method via sklearn is very sparsely implemented\n",
    "        # takes almost no method arguments\n",
    "        df_encoded = pd.get_dummies(self.dataset, drop_first=True)\n",
    "\n",
    "        #TODO: get columns from df_encoded if categorical data that is one hot encoded\n",
    "        \n",
    "        if label == None:\n",
    "            y = self.dataset[-1]\n",
    "        else:\n",
    "            y = self.dataset[label]\n",
    "            \n",
    "        if columns == None:\n",
    "            #assume all other columns and set X_columns to all features not label\n",
    "            X_columns = [col for col in df_encoded.columns if label not in col]\n",
    "        else:\n",
    "            # use only given columns\n",
    "            X_columns = columns\n",
    "        X = df_encoded[X_columns]\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "        self.model = LinearRegression()\n",
    "        self.model.fit(X_train, y_train)\n",
    "        self.accuracy = self.model.score(X_test, y_test)\n",
    "        print(f\"Model Accuracy: {self.accuracy}\")\n",
    "        return self.model\n",
    "\n",
    "    def svm(self,label=None, lr=1e-4, test_size=0.25, random_state=42, columns=None, max_epochs=100, verbose=1):\n",
    "        if label == None:\n",
    "            y = self.dataset[-1]\n",
    "        else:\n",
    "            y = self.dataset[label]\n",
    "            \n",
    "        X_columns = None\n",
    "        if columns == None:\n",
    "            #assume all other columns and set X_columns to all features not label\n",
    "            X_columns = [col for col in self.dataset.columns if label not in col]\n",
    "        else:\n",
    "            X_columns = columns\n",
    "            \n",
    "        X = self.dataset[X_columns]\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = test_size, random_state = random_state)\n",
    "        sc = StandardScaler()\n",
    "        X_train = sc.fit_transform(X_train)\n",
    "        X_test = sc.transform(X_test)\n",
    "        self.model = SVC(kernel = 'rbf', random_state = random_state, verbose = verbose)\n",
    "        self.model.fit(X_train, y_train)\n",
    "        y_pred = self.model.predict(X_test)\n",
    "        self.accuracy = accuracy_score(y_test,y_pred)\n",
    "        print(f\"Model Accuracy: {self.accuracy}\")\n",
    "        return self.model\n",
    "\n",
    "    def decision_tree(self,label=None, lr=1e-4, test_size=0.25, random_state=42, columns=None, max_epochs=100, verbose=1):\n",
    "        if label == None:\n",
    "            y = self.dataset[-1]\n",
    "        else:\n",
    "            y = self.dataset[label]\n",
    "        if columns == None:\n",
    "            #assume all other columns and set X_columns to all features not label\n",
    "            X_columns = [col for col in self.dataset.columns if label not in col]\n",
    "        else:\n",
    "            # use only given columns\n",
    "            X_columns = columns\n",
    "        X = self.dataset[X_columns]\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "\n",
    "        scaler = StandardScaler()\n",
    "        try:\n",
    "            X_train_scaled = scaler.fit_transform(X_train)\n",
    "        except:\n",
    "            X_train_scaled = X_train\n",
    "\n",
    "        try:\n",
    "            X_test_scaled = scaler.transform(X_test)\n",
    "        except:\n",
    "            X_test_scaled = X_test\n",
    "\n",
    "        #TODO: Add verbosity to decision tree classifier?\n",
    "        self.model = DecisionTreeClassifier(random_state = random_state)\n",
    "        self.model.fit(X_train_scaled, y_train)\n",
    "        y_pred = self.model.predict(X_test_scaled)\n",
    "        self.accuracy = self.model.score(X_test_scaled, y_test)\n",
    "        print(f\"Model Accuracy: {self.accuracy}\")\n",
    "        return self.model\n",
    "\n",
    "    def mlpClassifier(self,label=None, lr=1e-4, test_size=0.25, random_state=42, columns=None, max_epochs=100, verbose=1):\n",
    "        if label == None:\n",
    "            y = self.dataset[-1]\n",
    "        else:\n",
    "            y = self.dataset[label]\n",
    "        if columns == None:\n",
    "            #assume all other columns and set X_columns to all features not label\n",
    "            X_columns = [col for col in self.dataset.columns if label not in col]\n",
    "        else:\n",
    "            # use only given columns\n",
    "            X_columns = columns\n",
    "        X = self.dataset[X_columns]\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "\n",
    "        scaler = StandardScaler()\n",
    "\n",
    "        try:\n",
    "            X_train_scaled = scaler.fit_transform(X_train)\n",
    "            X_test_scaled = scaler.transform(X_test)\n",
    "        except:\n",
    "            X_train_scaled = X_train\n",
    "            X_test_scaled = X_test   \n",
    "        \n",
    "        self.model = MLPClassifier(random_state=1, max_iter=300).fit(X_train_scaled, y_train)\n",
    "        y_pred = self.model.predict(X_test_scaled)\n",
    "        self.accuracy = self.model.score(X_test_scaled, y_test)\n",
    "        print(f\"Model Accuracy: {self.accuracy}\")\n",
    "        return self.model\n",
    "\n",
    "    def mlpRegressor(self,label=None, lr=1e-4, test_size=0.25, random_state=42, columns=None, max_epochs=100, verbose=1):\n",
    "        if label == None:\n",
    "            y = self.dataset[-1]\n",
    "        else:\n",
    "            y = self.dataset[label]\n",
    "        if columns == None:\n",
    "            #assume all other columns and set X_columns to all features not label\n",
    "            X_columns = [col for col in self.dataset.columns if label not in col]\n",
    "        else:\n",
    "            # use only given columns\n",
    "            X_columns = columns\n",
    "        X = self.dataset[X_columns]\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "\n",
    "        scaler = StandardScaler()\n",
    "\n",
    "        try:\n",
    "            X_train_scaled = scaler.fit_transform(X_train)\n",
    "            X_test_scaled = scaler.transform(X_test)\n",
    "        except:\n",
    "            X_train_scaled = X_train\n",
    "            X_test_scaled = X_test            \n",
    "            \n",
    "        self.model = MLPRegressor(random_state=1, max_iter=300).fit(X_train_scaled, y_train)\n",
    "        y_pred = self.model.predict(X_test_scaled)\n",
    "        self.accuracy = self.model.score(X_test_scaled, y_test)\n",
    "        print(f\"Model Accuracy: {self.accuracy}\")\n",
    "        return self.model\n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--dataset DATASET] [--concat]\n",
      "                             [--task {logistic_regression,linear_regression,svm,decision_tree,recommend_model}]\n",
      "                             [--label LABEL] [--columns [COLUMNS ...]]\n",
      "                             [--test_size TEST_SIZE]\n",
      "                             [--random_state RANDOM_STATE]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f /Users/bcong/Library/Jupyter/runtime/kernel-c8f32d18-9e76-4e78-a1bb-12aff8a16918.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/IPython/core/interactiveshell.py:3516: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser(description=\"Run machine learning tasks with the MLAPI class.\")\n",
    "    parser.add_argument('--dataset', type=str, help=\"Path to the local CSV dataset.\")\n",
    "    parser.add_argument(\"--concat\", default=False, action=\"store_true\", help=\"Concat dataset flag\")\n",
    "    parser.add_argument('--task', type=str, choices=['logistic_regression', 'linear_regression', 'svm', 'decision_tree', 'recommend_model'], \n",
    "                        help=\"The machine learning task to run.\")\n",
    "    parser.add_argument('--label', type=str, help=\"The label column for training.\")\n",
    "    parser.add_argument('--columns', type=str, nargs='*', help=\"Feature columns for the model.\")\n",
    "    parser.add_argument('--test_size', type=float, default=0.25, help=\"Test size for the train/test split.\")\n",
    "    parser.add_argument('--random_state', type=int, default=42, help=\"Random state for reproducibility.\")\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # Create an instance of the class\n",
    "    ml_api = MLAPI()\n",
    "\n",
    "    # Load the dataset\n",
    "    if args.dataset:\n",
    "        if args.concat:\n",
    "            ml_api.set_local_csv_dataset(dataset= args.dataset, concat=True)\n",
    "        else:\n",
    "            ml_api.set_local_csv_dataset(dataset=args.dataset)\n",
    "    else:\n",
    "        if args.concat:\n",
    "            ml_api.set_local_csv_dataset(concat=True)\n",
    "        else:\n",
    "            ml_api.set_local_csv_dataset()\n",
    "\n",
    "    # Run the specified task\n",
    "    if args.task == 'logistic_regression':\n",
    "        ml_api.logistic_regression(label=args.label, columns=args.columns, test_size=args.test_size, random_state=args.random_state)\n",
    "    elif args.task == 'linear_regression':\n",
    "        ml_api.linear_regression(label=args.label, columns=args.columns, test_size=args.test_size, random_state=args.random_state)\n",
    "    elif args.task == 'svm':\n",
    "        ml_api.svm(label=args.label, columns=args.columns, test_size=args.test_size, random_state=args.random_state)\n",
    "    elif args.task == 'decision_tree':\n",
    "        ml_api.decision_tree(label=args.label, columns=args.columns, test_size=args.test_size, random_state=args.random_state)\n",
    "    elif args.task == 'recommend_model':\n",
    "        ml_api.recommed_model(columns=args.columns, features=args.label)\n",
    "    else:\n",
    "        print(\"Invalid task specified. Use --help for options.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook ML_API.ipynb to script\n",
      "[NbConvertApp] Writing 15185 bytes to ML_API.py\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbconvert --to script 'ML_API.ipynb'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "L4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
